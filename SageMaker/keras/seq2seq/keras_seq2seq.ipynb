{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-10 04:37:10--  http://www.manythings.org/anki/fra-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 172.67.173.198, 104.24.108.196, 104.24.109.196, ...\n",
      "Connecting to www.manythings.org (www.manythings.org)|172.67.173.198|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6041598 (5.8M) [application/zip]\n",
      "Saving to: ‘fra-eng.zip’\n",
      "\n",
      "fra-eng.zip         100%[===================>]   5.76M  4.78MB/s    in 1.2s    \n",
      "\n",
      "2020-07-10 04:37:12 (4.78 MB/s) - ‘fra-eng.zip’ saved [6041598/6041598]\n",
      "\n",
      "Archive:  fra-eng.zip\n",
      "  inflating: _about.txt              \n",
      "  inflating: fra.txt                 \n"
     ]
    }
   ],
   "source": [
    "!wget http://www.manythings.org/anki/fra-eng.zip\n",
    "!unzip fra-eng.zip\n",
    "os.makedirs(\"./data\", exist_ok = True)\n",
    "!mv fra.txt data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data is uploaded to: s3://sagemaker-ap-northeast-1-889905360474/data/handson-byom-tensorflow-keras\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "input_data = sagemaker_session.upload_data(path='./data', bucket=bucket_name, key_prefix='data/handson-byom-tensorflow-keras')\n",
    "print('Input data is uploaded to: {}'.format(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-10 04:36:09--  https://raw.githubusercontent.com/keras-team/keras/master/examples/lstm_seq2seq.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9348 (9.1K) [text/plain]\n",
      "Saving to: ‘lstm_seq2seq.py’\n",
      "\n",
      "lstm_seq2seq.py     100%[===================>]   9.13K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-07-10 04:36:09 (70.9 MB/s) - ‘lstm_seq2seq.py’ saved [9348/9348]\n",
      "\n",
      "--2020-07-10 04:36:09--  http://./\n",
      "Resolving . (.)... failed: No address associated with hostname.\n",
      "wget: unable to resolve host address ‘.’\n",
      "FINISHED --2020-07-10 04:36:09--\n",
      "Total wall clock time: 0.3s\n",
      "Downloaded: 1 files, 9.1K in 0s (70.9 MB/s)\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/keras-team/keras/master/examples/lstm_seq2seq.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-10 04:39:15 Starting - Starting the training job...\n",
      "2020-07-10 04:39:17 Starting - Launching requested ML instances.........\n",
      "2020-07-10 04:41:01 Starting - Preparing the instances for training...\n",
      "2020-07-10 04:41:39 Downloading - Downloading input data...\n",
      "2020-07-10 04:42:06 Training - Training image download completed. Training in progress..\u001b[34m2020-07-10 04:42:08,487 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-07-10 04:42:08,492 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-10 04:42:08,754 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-10 04:42:08,768 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-10 04:42:08,777 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 64,\n",
      "        \"num-classes\": 10,\n",
      "        \"model_dir\": \"s3://sagemaker-ap-northeast-1-889905360474/sagemaker-tensorflow-scriptmode-2020-07-10-04-39-14-897/model\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-tensorflow-scriptmode-2020-07-10-04-39-14-897\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-northeast-1-889905360474/sagemaker-tensorflow-scriptmode-2020-07-10-04-39-14-897/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"lstm_seq2seq\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"lstm_seq2seq.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":64,\"epochs\":1,\"model_dir\":\"s3://sagemaker-ap-northeast-1-889905360474/sagemaker-tensorflow-scriptmode-2020-07-10-04-39-14-897/model\",\"num-classes\":10}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=lstm_seq2seq.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=lstm_seq2seq\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-northeast-1-889905360474/sagemaker-tensorflow-scriptmode-2020-07-10-04-39-14-897/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":64,\"epochs\":1,\"model_dir\":\"s3://sagemaker-ap-northeast-1-889905360474/sagemaker-tensorflow-scriptmode-2020-07-10-04-39-14-897/model\",\"num-classes\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-tensorflow-scriptmode-2020-07-10-04-39-14-897\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-1-889905360474/sagemaker-tensorflow-scriptmode-2020-07-10-04-39-14-897/source/sourcedir.tar.gz\",\"module_name\":\"lstm_seq2seq\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"lstm_seq2seq.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"64\",\"--epochs\",\"1\",\"--model_dir\",\"s3://sagemaker-ap-northeast-1-889905360474/sagemaker-tensorflow-scriptmode-2020-07-10-04-39-14-897/model\",\"--num-classes\",\"10\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_NUM-CLASSES=10\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-ap-northeast-1-889905360474/sagemaker-tensorflow-scriptmode-2020-07-10-04-39-14-897/model\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python lstm_seq2seq.py --batch-size 64 --epochs 1 --model_dir s3://sagemaker-ap-northeast-1-889905360474/sagemaker-tensorflow-scriptmode-2020-07-10-04-39-14-897/model --num-classes 10\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[34mNumber of samples: 10000\u001b[0m\n",
      "\u001b[34mNumber of unique input tokens: 71\u001b[0m\n",
      "\u001b[34mNumber of unique output tokens: 92\u001b[0m\n",
      "\u001b[34mMax sequence length for inputs: 16\u001b[0m\n",
      "\u001b[34mMax sequence length for outputs: 59\u001b[0m\n",
      "\u001b[34mTrain on 8000 samples, validate on 2000 samples\u001b[0m\n",
      "\u001b[34mEpoch 1/1\u001b[0m\n",
      "\u001b[34m  64/8000 [..............................] - ETA: 5:44 - loss: 4.4922 - acc: 0.0050\n",
      " 128/8000 [..............................] - ETA: 3:08 - loss: 4.3146 - acc: 0.3661\u001b[0m\n",
      "\u001b[34m 192/8000 [..............................] - ETA: 2:16 - loss: 3.6928 - acc: 0.4881\n",
      " 256/8000 [..............................] - ETA: 1:50 - loss: 3.2888 - acc: 0.5436\n",
      " 320/8000 [>.............................] - ETA: 1:34 - loss: 2.9220 - acc: 0.5811\u001b[0m\n",
      "\u001b[34m 384/8000 [>.............................] - ETA: 1:24 - loss: 2.6755 - acc: 0.6057\n",
      " 448/8000 [>.............................] - ETA: 1:16 - loss: 2.4965 - acc: 0.6231\n",
      " 512/8000 [>.............................] - ETA: 1:10 - loss: 2.3693 - acc: 0.6343\n",
      " 576/8000 [=>............................] - ETA: 1:05 - loss: 2.2586 - acc: 0.6454\u001b[0m\n",
      "\u001b[34m 640/8000 [=>............................] - ETA: 1:01 - loss: 2.1717 - acc: 0.6533\n",
      " 704/8000 [=>............................] - ETA: 59s - loss: 2.1027 - acc: 0.6588 \n",
      " 768/8000 [=>............................] - ETA: 56s - loss: 2.0434 - acc: 0.6635\u001b[0m\n",
      "\u001b[34m 832/8000 [==>...........................] - ETA: 54s - loss: 1.9925 - acc: 0.6673\n",
      " 896/8000 [==>...........................] - ETA: 52s - loss: 1.9473 - acc: 0.6708\n",
      " 960/8000 [==>...........................] - ETA: 50s - loss: 1.9034 - acc: 0.6743\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 49s - loss: 1.8711 - acc: 0.6774\u001b[0m\n",
      "\u001b[34m1088/8000 [===>..........................] - ETA: 47s - loss: 1.8665 - acc: 0.6802\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 46s - loss: 1.8525 - acc: 0.6823\u001b[0m\n",
      "\u001b[34m1216/8000 [===>..........................] - ETA: 44s - loss: 1.8306 - acc: 0.6837\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 43s - loss: 1.8010 - acc: 0.6865\u001b[0m\n",
      "\u001b[34m1344/8000 [====>.........................] - ETA: 42s - loss: 1.7800 - acc: 0.6878\u001b[0m\n",
      "\u001b[34m1408/8000 [====>.........................] - ETA: 41s - loss: 1.7578 - acc: 0.6894\u001b[0m\n",
      "\u001b[34m1472/8000 [====>.........................] - ETA: 40s - loss: 1.7367 - acc: 0.6908\u001b[0m\n",
      "\u001b[34m1536/8000 [====>.........................] - ETA: 39s - loss: 1.7189 - acc: 0.6918\u001b[0m\n",
      "\u001b[34m1600/8000 [=====>........................] - ETA: 39s - loss: 1.6978 - acc: 0.6937\u001b[0m\n",
      "\u001b[34m1664/8000 [=====>........................] - ETA: 38s - loss: 1.6807 - acc: 0.6947\u001b[0m\n",
      "\u001b[34m1728/8000 [=====>........................] - ETA: 37s - loss: 1.6630 - acc: 0.6960\u001b[0m\n",
      "\u001b[34m1792/8000 [=====>........................] - ETA: 36s - loss: 1.6500 - acc: 0.6967\u001b[0m\n",
      "\u001b[34m1856/8000 [=====>........................] - ETA: 36s - loss: 1.6412 - acc: 0.6978\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 35s - loss: 1.6329 - acc: 0.6991\u001b[0m\n",
      "\u001b[34m1984/8000 [======>.......................] - ETA: 35s - loss: 1.6231 - acc: 0.6995\u001b[0m\n",
      "\u001b[34m2048/8000 [======>.......................] - ETA: 34s - loss: 1.6098 - acc: 0.7004\u001b[0m\n",
      "\u001b[34m2112/8000 [======>.......................] - ETA: 33s - loss: 1.5978 - acc: 0.7010\u001b[0m\n",
      "\u001b[34m2176/8000 [=======>......................] - ETA: 33s - loss: 1.5858 - acc: 0.7015\u001b[0m\n",
      "\u001b[34m2240/8000 [=======>......................] - ETA: 32s - loss: 1.5738 - acc: 0.7021\u001b[0m\n",
      "\u001b[34m2304/8000 [=======>......................] - ETA: 32s - loss: 1.5626 - acc: 0.7028\u001b[0m\n",
      "\u001b[34m2368/8000 [=======>......................] - ETA: 31s - loss: 1.5531 - acc: 0.7036\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 31s - loss: 1.5453 - acc: 0.7042\u001b[0m\n",
      "\u001b[34m2496/8000 [========>.....................] - ETA: 30s - loss: 1.5409 - acc: 0.7049\u001b[0m\n",
      "\u001b[34m2560/8000 [========>.....................] - ETA: 30s - loss: 1.5299 - acc: 0.7057\u001b[0m\n",
      "\u001b[34m2624/8000 [========>.....................] - ETA: 29s - loss: 1.5202 - acc: 0.7063\u001b[0m\n",
      "\u001b[34m2688/8000 [=========>....................] - ETA: 29s - loss: 1.5109 - acc: 0.7065\u001b[0m\n",
      "\u001b[34m2752/8000 [=========>....................] - ETA: 28s - loss: 1.5004 - acc: 0.7071\u001b[0m\n",
      "\u001b[34m2816/8000 [=========>....................] - ETA: 28s - loss: 1.4918 - acc: 0.7073\u001b[0m\n",
      "\u001b[34m2880/8000 [=========>....................] - ETA: 27s - loss: 1.4844 - acc: 0.7075\u001b[0m\n",
      "\u001b[34m2944/8000 [==========>...................] - ETA: 27s - loss: 1.4797 - acc: 0.7079\u001b[0m\n",
      "\u001b[34m3008/8000 [==========>...................] - ETA: 26s - loss: 1.4733 - acc: 0.7081\u001b[0m\n",
      "\u001b[34m3072/8000 [==========>...................] - ETA: 26s - loss: 1.4664 - acc: 0.7084\u001b[0m\n",
      "\u001b[34m3136/8000 [==========>...................] - ETA: 26s - loss: 1.4594 - acc: 0.7087\u001b[0m\n",
      "\u001b[34m3200/8000 [===========>..................] - ETA: 25s - loss: 1.4520 - acc: 0.7092\u001b[0m\n",
      "\u001b[34m3264/8000 [===========>..................] - ETA: 25s - loss: 1.4458 - acc: 0.7092\u001b[0m\n",
      "\u001b[34m3328/8000 [===========>..................] - ETA: 24s - loss: 1.4380 - acc: 0.7099\u001b[0m\n",
      "\u001b[34m3392/8000 [===========>..................] - ETA: 24s - loss: 1.4314 - acc: 0.7102\u001b[0m\n",
      "\u001b[34m3456/8000 [===========>..................] - ETA: 23s - loss: 1.4256 - acc: 0.7104\u001b[0m\n",
      "\u001b[34m3520/8000 [============>.................] - ETA: 23s - loss: 1.4211 - acc: 0.7103\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 23s - loss: 1.4142 - acc: 0.7110\u001b[0m\n",
      "\u001b[34m3648/8000 [============>.................] - ETA: 22s - loss: 1.4082 - acc: 0.7115\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 22s - loss: 1.4038 - acc: 0.7117\u001b[0m\n",
      "\u001b[34m3776/8000 [=============>................] - ETA: 21s - loss: 1.3994 - acc: 0.7118\u001b[0m\n",
      "\u001b[34m3840/8000 [=============>................] - ETA: 21s - loss: 1.3952 - acc: 0.7121\u001b[0m\n",
      "\u001b[34m3904/8000 [=============>................] - ETA: 21s - loss: 1.3898 - acc: 0.7125\u001b[0m\n",
      "\u001b[34m3968/8000 [=============>................] - ETA: 20s - loss: 1.3837 - acc: 0.7131\u001b[0m\n",
      "\u001b[34m4032/8000 [==============>...............] - ETA: 20s - loss: 1.3778 - acc: 0.7135\u001b[0m\n",
      "\u001b[34m4096/8000 [==============>...............] - ETA: 20s - loss: 1.3731 - acc: 0.7137\u001b[0m\n",
      "\u001b[34m4160/8000 [==============>...............] - ETA: 19s - loss: 1.3678 - acc: 0.7141\u001b[0m\n",
      "\u001b[34m4224/8000 [==============>...............] - ETA: 19s - loss: 1.3643 - acc: 0.7143\u001b[0m\n",
      "\u001b[34m4288/8000 [===============>..............] - ETA: 19s - loss: 1.3604 - acc: 0.7144\u001b[0m\n",
      "\u001b[34m4352/8000 [===============>..............] - ETA: 18s - loss: 1.3550 - acc: 0.7149\u001b[0m\n",
      "\u001b[34m4416/8000 [===============>..............] - ETA: 18s - loss: 1.3509 - acc: 0.7150\u001b[0m\n",
      "\u001b[34m4480/8000 [===============>..............] - ETA: 17s - loss: 1.3466 - acc: 0.7152\u001b[0m\n",
      "\u001b[34m4544/8000 [================>.............] - ETA: 17s - loss: 1.3415 - acc: 0.7155\u001b[0m\n",
      "\u001b[34m4608/8000 [================>.............] - ETA: 17s - loss: 1.3367 - acc: 0.7158\u001b[0m\n",
      "\u001b[34m4672/8000 [================>.............] - ETA: 16s - loss: 1.3318 - acc: 0.7162\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 16s - loss: 1.3285 - acc: 0.7163\u001b[0m\n",
      "\u001b[34m4800/8000 [=================>............] - ETA: 16s - loss: 1.3258 - acc: 0.7164\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 15s - loss: 1.3224 - acc: 0.7168\u001b[0m\n",
      "\u001b[34m4928/8000 [=================>............] - ETA: 15s - loss: 1.3183 - acc: 0.7171\u001b[0m\n",
      "\u001b[34m4992/8000 [=================>............] - ETA: 15s - loss: 1.3144 - acc: 0.7173\u001b[0m\n",
      "\u001b[34m5056/8000 [=================>............] - ETA: 14s - loss: 1.3102 - acc: 0.7177\u001b[0m\n",
      "\u001b[34m5120/8000 [==================>...........] - ETA: 14s - loss: 1.3064 - acc: 0.7180\u001b[0m\n",
      "\u001b[34m5184/8000 [==================>...........] - ETA: 14s - loss: 1.3035 - acc: 0.7180\u001b[0m\n",
      "\u001b[34m5248/8000 [==================>...........] - ETA: 13s - loss: 1.3000 - acc: 0.7182\u001b[0m\n",
      "\u001b[34m5312/8000 [==================>...........] - ETA: 13s - loss: 1.2962 - acc: 0.7184\u001b[0m\n",
      "\u001b[34m5376/8000 [===================>..........] - ETA: 13s - loss: 1.2926 - acc: 0.7185\u001b[0m\n",
      "\u001b[34m5440/8000 [===================>..........] - ETA: 12s - loss: 1.2883 - acc: 0.7188\u001b[0m\n",
      "\u001b[34m5504/8000 [===================>..........] - ETA: 12s - loss: 1.2847 - acc: 0.7191\u001b[0m\n",
      "\u001b[34m5568/8000 [===================>..........] - ETA: 12s - loss: 1.2816 - acc: 0.7192\u001b[0m\n",
      "\u001b[34m5632/8000 [====================>.........] - ETA: 11s - loss: 1.2782 - acc: 0.7193\u001b[0m\n",
      "\u001b[34m5696/8000 [====================>.........] - ETA: 11s - loss: 1.2759 - acc: 0.7195\u001b[0m\n",
      "\u001b[34m5760/8000 [====================>.........] - ETA: 11s - loss: 1.2778 - acc: 0.7195\u001b[0m\n",
      "\u001b[34m5824/8000 [====================>.........] - ETA: 10s - loss: 1.2746 - acc: 0.7199\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 10s - loss: 1.2718 - acc: 0.7201\u001b[0m\n",
      "\u001b[34m5952/8000 [=====================>........] - ETA: 10s - loss: 1.2685 - acc: 0.7203\u001b[0m\n",
      "\u001b[34m6016/8000 [=====================>........] - ETA: 9s - loss: 1.2658 - acc: 0.7204 \u001b[0m\n",
      "\u001b[34m6080/8000 [=====================>........] - ETA: 9s - loss: 1.2634 - acc: 0.7205\u001b[0m\n",
      "\u001b[34m6144/8000 [======================>.......] - ETA: 9s - loss: 1.2608 - acc: 0.7206\u001b[0m\n",
      "\u001b[34m6208/8000 [======================>.......] - ETA: 8s - loss: 1.2584 - acc: 0.7207\u001b[0m\n",
      "\u001b[34m6272/8000 [======================>.......] - ETA: 8s - loss: 1.2555 - acc: 0.7208\u001b[0m\n",
      "\u001b[34m6336/8000 [======================>.......] - ETA: 8s - loss: 1.2530 - acc: 0.7209\u001b[0m\n",
      "\u001b[34m6400/8000 [=======================>......] - ETA: 7s - loss: 1.2505 - acc: 0.7210\u001b[0m\n",
      "\u001b[34m6464/8000 [=======================>......] - ETA: 7s - loss: 1.2479 - acc: 0.7211\u001b[0m\n",
      "\u001b[34m6528/8000 [=======================>......] - ETA: 7s - loss: 1.2459 - acc: 0.7212\u001b[0m\n",
      "\u001b[34m6592/8000 [=======================>......] - ETA: 6s - loss: 1.2430 - acc: 0.7216\u001b[0m\n",
      "\u001b[34m6656/8000 [=======================>......] - ETA: 6s - loss: 1.2409 - acc: 0.7216\u001b[0m\n",
      "\u001b[34m6720/8000 [========================>.....] - ETA: 6s - loss: 1.2388 - acc: 0.7217\u001b[0m\n",
      "\u001b[34m6784/8000 [========================>.....] - ETA: 5s - loss: 1.2363 - acc: 0.7218\u001b[0m\n",
      "\u001b[34m6848/8000 [========================>.....] - ETA: 5s - loss: 1.2344 - acc: 0.7218\u001b[0m\n",
      "\u001b[34m6912/8000 [========================>.....] - ETA: 5s - loss: 1.2316 - acc: 0.7220\u001b[0m\n",
      "\u001b[34m6976/8000 [=========================>....] - ETA: 5s - loss: 1.2295 - acc: 0.7221\u001b[0m\n",
      "\u001b[34m7040/8000 [=========================>....] - ETA: 4s - loss: 1.2271 - acc: 0.7222\u001b[0m\n",
      "\u001b[34m7104/8000 [=========================>....] - ETA: 4s - loss: 1.2248 - acc: 0.7224\u001b[0m\n",
      "\u001b[34m7168/8000 [=========================>....] - ETA: 4s - loss: 1.2227 - acc: 0.7225\u001b[0m\n",
      "\u001b[34m7232/8000 [==========================>...] - ETA: 3s - loss: 1.2206 - acc: 0.7226\u001b[0m\n",
      "\u001b[34m7296/8000 [==========================>...] - ETA: 3s - loss: 1.2185 - acc: 0.7228\u001b[0m\n",
      "\u001b[34m7360/8000 [==========================>...] - ETA: 3s - loss: 1.2161 - acc: 0.7229\u001b[0m\n",
      "\u001b[34m7424/8000 [==========================>...] - ETA: 2s - loss: 1.2139 - acc: 0.7231\u001b[0m\n",
      "\u001b[34m7488/8000 [===========================>..] - ETA: 2s - loss: 1.2120 - acc: 0.7231\u001b[0m\n",
      "\u001b[34m7552/8000 [===========================>..] - ETA: 2s - loss: 1.2106 - acc: 0.7231\u001b[0m\n",
      "\u001b[34m7616/8000 [===========================>..] - ETA: 1s - loss: 1.2091 - acc: 0.7233\u001b[0m\n",
      "\u001b[34m7680/8000 [===========================>..] - ETA: 1s - loss: 1.2081 - acc: 0.7233\u001b[0m\n",
      "\u001b[34m7744/8000 [============================>.] - ETA: 1s - loss: 1.2063 - acc: 0.7234\u001b[0m\n",
      "\u001b[34m7808/8000 [============================>.] - ETA: 0s - loss: 1.2046 - acc: 0.7235\u001b[0m\n",
      "\u001b[34m7872/8000 [============================>.] - ETA: 0s - loss: 1.2032 - acc: 0.7235\u001b[0m\n",
      "\u001b[34m7936/8000 [============================>.] - ETA: 0s - loss: 1.2015 - acc: 0.7236\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 41s 5ms/step - loss: 1.1995 - acc: 0.7237 - val_loss: 1.0512 - val_acc: 0.7040\u001b[0m\n",
      "\u001b[34msave1\u001b[0m\n",
      "\u001b[34minput\u001b[0m\n",
      "\u001b[34mTensor(\"input_1:0\", shape=(?, ?, 71), dtype=float32)\u001b[0m\n",
      "\u001b[34moutput\u001b[0m\n",
      "\u001b[34mTensor(\"lstm_1/while/Exit_2:0\", shape=(?, 256), dtype=float32)\u001b[0m\n",
      "\u001b[34mTensor(\"lstm_1/while/Exit_3:0\", shape=(?, 256), dtype=float32)\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/simple_save.py:85: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPass your op to the equivalent parameter main_op instead.\u001b[0m\n",
      "\u001b[34msave2\u001b[0m\n",
      "\u001b[34minput\u001b[0m\n",
      "\u001b[34mTensor(\"input_2:0\", shape=(?, ?, 92), dtype=float32)\u001b[0m\n",
      "\u001b[34mTensor(\"input_3:0\", shape=(?, 256), dtype=float32)\u001b[0m\n",
      "\u001b[34mTensor(\"input_4:0\", shape=(?, 256), dtype=float32)\u001b[0m\n",
      "\u001b[34moutput\u001b[0m\n",
      "\u001b[34mTensor(\"dense_1_1/truediv:0\", shape=(?, ?, 92), dtype=float32)\u001b[0m\n",
      "\u001b[34mTensor(\"lstm_2_1/while/Exit_2:0\", shape=(?, 256), dtype=float32)\u001b[0m\n",
      "\u001b[34mTensor(\"lstm_2_1/while/Exit_3:0\", shape=(?, 256), dtype=float32)\u001b[0m\n",
      "\u001b[34m2020-07-10 04:42:53,760 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-07-10 04:43:05 Uploading - Uploading generated training model\n",
      "2020-07-10 04:43:05 Completed - Training job completed\n",
      "Training seconds: 86\n",
      "Billable seconds: 86\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker import get_execution_role\n",
    "train_instance_type = \"ml.c5.xlarge\"\n",
    "role = get_execution_role()\n",
    "estimator = TensorFlow(entry_point = \"./lstm_seq2seq.py\",\n",
    "                       role=role,\n",
    "                       train_instance_count=1,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       framework_version=\"1.12.0\",\n",
    "                       py_version='py3',\n",
    "                       script_mode=True,\n",
    "                       hyperparameters={'batch-size': 64,\n",
    "                                        'num-classes': 10,\n",
    "                                        'epochs': 100})\n",
    "estimator.fit(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "endpoint = predictor.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 92\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "    \n",
    "batch_size = 64  # Batch size for training.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'data/train.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "    \n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Predictor\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_predictor = Predictor(endpoint, model_name='model1')\n",
    "    states_value = (encoder_predictor.predict(input_seq))['predictions'][0]\n",
    "    h = states_value['lstm_1/while/Exit_2:0']\n",
    "    c = states_value['lstm_1/while/Exit_3:0']\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, num_decoder_tokens))#np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    decoder_predictor = Predictor(endpoint, model_name='model2')\n",
    "    while not stop_condition:\n",
    "        #output_tokens, h, c \n",
    "        decoder_result= (decoder_predictor.predict(\n",
    "            {\"instances\":[{ 'input_2:0':target_seq.tolist(), 'input_3:0':h, 'input_4:0':c}]}))['predictions'][0]\n",
    "        \n",
    "        output_tokens = decoder_result['dense_1_1/truediv:0']\n",
    "        h = decoder_result['lstm_2_1/while/Exit_2:0']\n",
    "        c = decoder_result['lstm_2_1/while/Exit_3:0']\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens)#output_tokens[0, -1, :]\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, num_decoder_tokens))\n",
    "        target_seq[0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Je                                                          \n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decode_sequence(input_seq)\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
